# â™Ÿï¸ Ajedrez con Q-Learning

Un sistema de ajedrez con aprendizaje por refuerzo (Q-Learning tabular) donde las blancas y las negras aprenden a jugar automÃ¡ticamente. El conocimiento se guarda automÃ¡ticamente en el navegador y puede exportarse/importarse.

## ğŸ“‹ CaracterÃ­sticas

- **2 agentes con IA**: Blancas y Negras aprenden simultÃ¡neamente
- **Q-Learning tabular**: Los agentes aprenden y mejoran con el tiempo
- **Tablero completo 8x8**: ImplementaciÃ³n completa de reglas de ajedrez
- **Persistencia automÃ¡tica**: El conocimiento se guarda en localStorage
- **ExportaciÃ³n/ImportaciÃ³n**: Guarda y carga el conocimiento aprendido
- **Panel de control interactivo**: Controla velocidad, parÃ¡metros de entrenamiento y mÃ¡s
- **MÃ©tricas en tiempo real**: Winrate, partidas, movimientos y tamaÃ±o de Q-table
- **Sin dependencias externas**: Solo HTML5 + JavaScript puro

## ğŸš€ EjecuciÃ³n Local

### OpciÃ³n 1: Servidor HTTP simple (Python)

```bash
cd ajedrez-ql
python -m http.server 8000
```

Luego abre tu navegador en `http://localhost:8000`

### OpciÃ³n 2: Servidor HTTP simple (Node.js)

```bash
cd ajedrez-ql
npx http-server -p 8000
```

Luego abre tu navegador en `http://localhost:8000`

### OpciÃ³n 3: Abrir directamente

Simplemente abre el archivo `index.html` en tu navegador.

## ğŸ“‚ Estructura del Proyecto

```
ajedrez-ql/
â”œâ”€â”€ index.html          # PÃ¡gina principal
â”œâ”€â”€ css/
â”‚   â””â”€â”€ styles.css      # Estilos de la interfaz
â”œâ”€â”€ js/
â”‚   â”œâ”€â”€ board.js        # Tablero de ajedrez y movimientos
â”‚   â”œâ”€â”€ agents.js       # Agentes con Q-Learning
â”‚   â”œâ”€â”€ training.js     # Sistema de entrenamiento
â”‚   â”œâ”€â”€ engine.js       # Motor de simulaciÃ³n
â”‚   â””â”€â”€ ui.js           # Panel de control y mÃ©tricas
â””â”€â”€ README.md           # Este archivo
```

## ğŸ¯ CÃ³mo Usar

1. **Iniciar/Pausar**: Presiona el botÃ³n "â–¶ï¸ Iniciar" o la tecla `Espacio`
2. **Reiniciar partida**: Presiona "ğŸ”„ Reset" o la tecla `R`
3. **Reset entrenamiento**: Presiona "ğŸ—‘ï¸ Reset Entrenamiento" o la tecla `T` (borra todo el aprendizaje)
4. **Ajustar velocidad**: Usa el slider o las teclas `â†‘`/`â†“` (1x - 100x)
5. **Exportar conocimiento**: Presiona "ğŸ“¥ Exportar Conocimiento" para descargar un JSON
6. **Importar conocimiento**: Presiona "ğŸ“¤ Importar Conocimiento" para cargar un JSON

## âš™ï¸ ParÃ¡metros de Entrenamiento

### ExploraciÃ³n (Îµ)
- Controla la probabilidad de explorar vs explotar
- Valores tÃ­picos: 0.05 - 0.3
- MÃ¡s alto = mÃ¡s exploraciÃ³n, mÃ¡s bajo = mÃ¡s explotaciÃ³n

### Tasa de Aprendizaje (Î±)
- Controla quÃ© tan rÃ¡pido se actualizan los valores Q
- Valores tÃ­picos: 0.05 - 0.2
- MÃ¡s alto = aprendizaje mÃ¡s rÃ¡pido, pero menos estable

### Factor de Descuento (Î³)
- Controla la importancia de recompensas futuras
- Valores tÃ­picos: 0.85 - 0.99
- MÃ¡s alto = mÃ¡s importancia a recompensas a largo plazo

### MÃ¡ximo de Movimientos
- LÃ­mite de movimientos por partida para evitar partidas infinitas
- Valores tÃ­picos: 100 - 500

## ğŸ§  Sistema de Aprendizaje

### Estado del Agente
Cada agente observa:
- Hash del tablero (representaciÃ³n compacta)
- Color a mover
- Diferencia material entre ambos equipos

### Recompensas

**Por acciÃ³n:**
- `+10 * valor de pieza` por capturar una pieza
- `+5` por dar jaque
- `+1000` por jaque mate
- `-100` por ahogado (stalemate)
- `+2 * diferencia material` por ventaja material
- `-0.1` por cada movimiento

### Reglas del Juego
- **Movimientos vÃ¡lidos**: Todas las piezas de ajedrez con sus movimientos estÃ¡ndar
- **PromociÃ³n de peÃ³n**: Los peones se promocionan automÃ¡ticamente a reina
- **Jaque mate**: Fin del juego cuando el rey estÃ¡ en jaque y no tiene movimientos vÃ¡lidos
- **Ahogado**: Fin del juego cuando el rey no estÃ¡ en jaque pero no tiene movimientos vÃ¡lidos
- **LÃ­mite de movimientos**: La partida termina si se alcanza el lÃ­mite configurado

## ğŸ’¾ Persistencia del Conocimiento

### localStorage
El conocimiento se guarda automÃ¡ticamente en el navegador en las siguientes claves:
- `chess_qtable_white`: Q-table de las blancas
- `chess_qtable_black`: Q-table de las negras

### Exportar/Importar
Puedes exportar el conocimiento aprendido a un archivo JSON y compartirlo con otros:
1. Haz clic en "ğŸ“¥ Exportar Conocimiento"
2. Se descargarÃ¡ un archivo `chess_knowledge_YYYY-MM-DD.json`
3. Para importar, haz clic en "ğŸ“¤ Importar Conocimiento" y selecciona el archivo

## ğŸ”§ PersonalizaciÃ³n

### Ajustar Recompensas

En [`js/agents.js`](js/agents.js:150):

```javascript
calculateReward(capturedPiece, isCheck, isCheckmate, isStalemate, materialDiff) {
    let reward = 0;
    if (capturedPiece) {
        const pieceValues = {
            pawn: 1, knight: 3, bishop: 3,
            rook: 5, queen: 9, king: 0
        };
        reward += pieceValues[capturedPiece.type] * 10;  // Ajusta aquÃ­
    }
    // ... resto del cÃ³digo
    return reward;
}
```

### Ajustar ParÃ¡metros de Entrenamiento

En [`js/training.js`](js/training.js:15):

```javascript
// ParÃ¡metros por defecto
this.epsilon = 0.1;           // Tasa de exploraciÃ³n
this.learningRate = 0.1;      // Tasa de aprendizaje
this.discount = 0.9;          // Factor de descuento
this.maxMoves = 200;          // LÃ­mite de movimientos
```

### Ajustar TamaÃ±o del Tablero

En [`js/board.js`](js/board.js:5):

```javascript
const BOARD_SIZE = 8;  // Cambia para diferentes tamaÃ±os
```

## ğŸ“Š MÃ©tricas Disponibles

- **Partida actual**: NÃºmero de la partida en curso
- **Movimiento actual**: NÃºmero de movimientos en la partida
- **Turno actual**: Color a mover (Blancas/Negras)
- **Total partidas**: NÃºmero total de partidas jugadas
- **Victorias blancas**: NÃºmero de partidas ganadas por blancas
- **Victorias negras**: NÃºmero de partidas ganadas por negras
- **Tablas**: NÃºmero de partidas terminadas en empate
- **Total movimientos**: NÃºmero total de movimientos ejecutados
- **Promedio movimientos/partida**: Promedio de movimientos por partida
- **Winrate blancas**: Porcentaje de victorias de blancas
- **Winrate negras**: Porcentaje de victorias de negras
- **TamaÃ±o Q-table**: NÃºmero de estados aprendidos por cada equipo
- **Total de entradas**: NÃºmero total de entradas Q por cada equipo

## ğŸ“ Conceptos de Aprendizaje por Refuerzo

### Q-Learning
El algoritmo Q-Learning aprende una funciÃ³n Q(s,a) que representa el valor esperado de tomar la acciÃ³n a en el estado s:

```
Q(s,a) = Q(s,a) + Î± * (r + Î³ * max(Q(s',a')) - Q(s,a))
```

Donde:
- `s`: Estado actual
- `a`: AcciÃ³n tomada
- `r`: Recompensa recibida
- `s'`: Nuevo estado
- `Î±`: Tasa de aprendizaje
- `Î³`: Factor de descuento

### PolÃ­tica Îµ-Greedy
Para equilibrar exploraciÃ³n y explotaciÃ³n:
- Con probabilidad Îµ: elegir acciÃ³n aleatoria (explorar)
- Con probabilidad 1-Îµ: elegir mejor acciÃ³n segÃºn Q-table (explotar)

## ğŸŒ Alojamiento como PÃ¡gina EstÃ¡tica

### GitHub Pages

1. Crea un repositorio en GitHub
2. Sube los archivos del proyecto
3. Ve a Settings > Pages
4. Selecciona la rama `main` y guarda
5. Tu sitio estarÃ¡ disponible en `https://tu-usuario.github.io/ajedrez-ql/`

### Netlify

1. Instala Netlify CLI: `npm install -g netlify-cli`
2. En la carpeta del proyecto: `netlify deploy --prod`
3. Sigue las instrucciones

### Vercel

1. Instala Vercel CLI: `npm install -g vercel`
2. En la carpeta del proyecto: `vercel --prod`
3. Sigue las instrucciones

## ğŸ”„ EvoluciÃ³n Futura

Posibles mejoras para el proyecto:

1. **Deep Q-Networks (DQN)**: Reemplazar Q-table tabular con una red neuronal
2. **AlphaZero-style**: Usar Monte Carlo Tree Search (MCTS) con redes neuronales
3. **Self-play mejorado**: Implementar tÃ©cnicas modernas de auto-aprendizaje
4. **Enroque**: Implementar movimientos de enroque
5. **Captura al paso**: Implementar la regla de captura al paso
6. **Torneos**: Modo torneo entre diferentes versiones de la IA

## ğŸ“ Notas TÃ©cnicas

### Espacio de Estados
El espacio de estados de ajedrez es extremadamente grande (~10^43 estados posibles). Por esta razÃ³n:
- Se usa una representaciÃ³n simplificada del estado
- El aprendizaje es mÃ¡s lento que en juegos mÃ¡s simples
- Se recomienda entrenar por muchas partidas (miles o millones)

### Rendimiento
- El Q-Learning tabular puede consumir mucha memoria
- Se recomienda usar un navegador moderno con suficiente RAM
- La velocidad de entrenamiento depende de la potencia del CPU

## ğŸ› SoluciÃ³n de Problemas

### El conocimiento no se guarda
- Verifica que tu navegador permita localStorage
- Limpia la cachÃ© del navegador si es necesario

### El juego es lento
- Reduce la velocidad en el slider
- Reduce el nÃºmero de movimientos por frame en [`js/engine.js`](js/engine.js:25)

### Los agentes no mejoran
- Aumenta la tasa de exploraciÃ³n (Îµ)
- Aumenta la tasa de aprendizaje (Î±)
- Entrena por mÃ¡s tiempo (miles de partidas)

## ğŸ“„ Licencia

Este proyecto es de cÃ³digo abierto y estÃ¡ disponible para uso educativo y personal.

## ğŸ‘¨â€ğŸ’» Autor

Desarrollado como prototipo de aprendizaje por refuerzo en ajedrez.

## ğŸ™ Agradecimientos

- Inspirado en proyectos de ajedrez con IA
- Basado en algoritmos de Q-Learning estÃ¡ndar
- Implementado con HTML5 Canvas y JavaScript puro

---

**Â¡Disfruta experimentando con el aprendizaje por refuerzo en ajedrez! â™Ÿï¸ğŸ§ **
